{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Probability and Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Expectation, Variance and Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x, E[x]:  tensor([1., 2., 3., 4.]) tensor(2.5000)\n",
      "f(x), E[f(x)]:  tensor([ 1.,  4.,  9., 16.]) tensor(7.5000)\n",
      "g(x), E[g(x)]:  tensor([ 3.,  6.,  9., 12.]) tensor(7.5000)\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "def g(x):\n",
    "    return x * 3\n",
    "\n",
    "p_x = torch.Tensor([0.25, 0.25, 0.25, 0.25])\n",
    "x = torch.Tensor([1, 2, 3, 4])\n",
    "fx = f(x)\n",
    "gx = g(x)\n",
    "\n",
    "E_x = p_x.dot(x)\n",
    "E_fx = p_x.dot(fx)\n",
    "E_gx = p_x.dot(gx)\n",
    "\n",
    "print(\"x, E[x]: \", x, E_x)\n",
    "print(\"f(x), E[f(x)]: \", fx, E_fx)\n",
    "print(\"g(x), E[g(x)]: \", gx, E_gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[f(x)] + E[g(x)]:  tensor(15.)\n",
      "E[f(x) + g(x)]:  tensor(15.)\n"
     ]
    }
   ],
   "source": [
    "fx_gx = f(x) + g(x)\n",
    "E_fx_gx = p_x.dot(fx_gx)\n",
    "\n",
    "print(\"E[f(x)] + E[g(x)]: \", E_fx + E_gx)\n",
    "print(\"E[f(x) + g(x)]: \", E_fx_gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  4.,  9., 16.]) tensor(7.5000)\n",
      "tensor(32.2500)\n",
      "tensor(43.)\n"
     ]
    }
   ],
   "source": [
    "Var_fx = ((fx - E_fx) ** 2).dot(p_x)\n",
    "print(fx, E_fx)\n",
    "print(Var_fx)\n",
    "print(fx.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you see our result and the result given by PyTorch are different.\n",
    "\n",
    "Variance is known to be underestimated when the number of sample is small, so basically we use (n-1) instead of n to bias. PyTorch uses n-1 and we used n, which made this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
